{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from keras.models import Sequential, load_model, Model \n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Input, BatchNormalization, Activation, add, MaxPooling1D, Cropping1D \n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "#from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "#import hyperopt.fmin as hypfmin\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from math import ceil\n",
    "from Bio import SeqIO\n",
    "import warnings \n",
    "import datetime\n",
    "import keras.backend as kb\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "import pybedtools as bt\n",
    "import h5py\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.core import Lambda\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readM6A(File_list):\n",
    "    tabs=pd.DataFrame()\n",
    "    for f in File_list:\n",
    "        tab=pd.read_csv(f,sep='\\t',comment='#',header=None)\n",
    "        tabs=tabs.append(tab,ignore_index=True)\n",
    "    tabs.drop_duplicates(subset=[0,1,2],keep='first',inplace=True)\n",
    "    return tabs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to add so as only take Pos samples that have RRACH \n",
    "def getInputMtxs_DNA(genomeFasta,sites,TrxtsGenomeFlank,chrs):\n",
    "    fasta_sequences = SeqIO.parse(open(genomeFasta),'fasta')\n",
    "    seqs={}\n",
    "    for seq in fasta_sequences:\n",
    "        if seq.name[:3]=='chr':\n",
    "            seqs[seq.name]=seq  \n",
    "    trxts=sites\n",
    "    trxts=trxts.copy()\n",
    "    chrs=['chr'+str(c) for c in chrs]\n",
    "    trxts=trxts[trxts[0].isin(chrs)]\n",
    "    trxts[1]=(trxts[2]-TrxtsGenomeFlank)\n",
    "    trxts[2]=(trxts[2]+TrxtsGenomeFlank)\n",
    "    trnMtxs=[]\n",
    "    negBases=dict(zip(['A','C','G','T'],range(4)))\n",
    "    posBases=dict(zip(['T','G','C','A'],range(4)))\n",
    "    for r in range(len(trxts)):\n",
    "        row=trxts.iloc[r,:]\n",
    "        trxtW=row[2]-row[1]+1\n",
    "        trxtSeq=seqs[row[0]].seq[row[1]-1:row[2]]\n",
    "        trnMtx = np.matrix(np.zeros((4,trxtW)))\n",
    "        if row[5] == '+':\n",
    "            for bi in range(trxtW):\n",
    "                try:\n",
    "                    b = trxtSeq[bi]\n",
    "                    trnMtx[posBases[b],bi] = 1\n",
    "                except:\n",
    "                    continue \n",
    "        else:\n",
    "            for bi in range(trxtW):        \n",
    "                try:\n",
    "                    b = trxtSeq[bi]            \n",
    "                    trnMtx[negBases[b],bi] = 1\n",
    "                except:\n",
    "                    continue \n",
    "            trnMtx = np.fliplr(trnMtx)\n",
    "        trnMtxs.append(trnMtx)\n",
    "    return trnMtxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifyMotif(genomeFasta,sites,motif):\n",
    "    fasta_sequences = SeqIO.parse(open(genomeFasta),'fasta')\n",
    "    seqs={}\n",
    "    for seq in fasta_sequences:\n",
    "        if seq.name[:3]=='chr':\n",
    "            seqs[seq.name]=seq \n",
    "    sites=sites.copy()\n",
    "    TrueInds = [] \n",
    "    for r in range(len(sites)):\n",
    "        row=sites.iloc[r,:]\n",
    "        i=row[1]\n",
    "        chrom = row[0]\n",
    "        if chrom not in seqs.keys():\n",
    "            continue \n",
    "        if row[5] == '+':\n",
    "            if seqs[chrom].seq[i] == 'A':\n",
    "                if seqs[chrom].seq[i+1] == 'C' :\n",
    "                    if seqs[chrom].seq[i-1] in ['A','G'] :\n",
    "                        if motif == 'RRACH':\n",
    "                            if seqs[chrom].seq[i-2] in ['A','G'] :\n",
    "                                if seqs[chrom].seq[i+2] in ['A','C','T'] :\n",
    "                                    TrueInds.append(r)\n",
    "                        elif motif == 'RAC' :\n",
    "                            TrueInds.append(r)\n",
    "        else:\n",
    "            if seqs[chrom].seq[i] == 'T':\n",
    "                if seqs[chrom].seq[i-1] == 'G' :\n",
    "                    if seqs[chrom].seq[i+1] in ['T','C'] :\n",
    "                        if motif == 'RRACH':\n",
    "                            if seqs[chrom].seq[i+2] in ['T','C'] :\n",
    "                                if seqs[chrom].seq[i-2] in ['T','G','A'] :\n",
    "                                    TrueInds.append(r)\n",
    "                        elif motif == 'RAC' :\n",
    "                            TrueInds.append(r)\n",
    "    return sites.iloc[TrueInds,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#WHICH TRANSCRIPTS WE USE COULD AFFECT WHICH SITES Neg TRAINING DATA IS ON and EFFECT OUTCOME --- may need to adjust this\n",
    "def processGff(AgFn) :\n",
    "    GCDE = pd.read_csv(AgFn,sep='\\t',comment='#',header=None)\n",
    "    trxts = GCDE[GCDE[2]==\"transcript\"]\n",
    "    pcgBool=[bool(re.search('gene_type \\\"(?:protein_coding|.*l.*ncRNA)\\\";',s)) for s in trxts[8]]\n",
    "    trxts=trxts[pcgBool]\n",
    "    trxts['trx_name']=trxts.iloc[:,8].str.extract('transcript_name \\\"(.+?)\\\";')\n",
    "    trxts['gene']=trxts.iloc[:,8].str.extract('gene_name \\\"(.+?)\\\";')\n",
    "    trxts['trxSz']=trxts[4]-trxts[3]\n",
    "    splitGps = trxts.groupby('gene')\n",
    "    dups=pd.DataFrame()\n",
    "    trxts=pd.DataFrame()\n",
    "    for sg in splitGps:\n",
    "        sg=sg[1]\n",
    "        if len(sg) > 1:\n",
    "            sg=sg[sg['trxSz'] == sg['trxSz'].max()]\n",
    "            if len(sg) > 1:\n",
    "                sg=sg.iloc[0:1,:]\n",
    "                if len(sg) > 1:\n",
    "                    dups=dups.append(sg)\n",
    "                    continue \n",
    "        trxts=trxts.append(sg)\n",
    "    if len(dups) > 0:\n",
    "        print(\"{} Removed (Still has duplicates!!!)\".format(dups['trx_name']))\n",
    "        print(\"{} gene(s) with duplicates removed!!!\".format(len(set(dups['gene']))))\n",
    "    cols=list(range(0,9))+['trx_name','gene']\n",
    "    trxts=trxts.loc[:,cols]\n",
    "    return trxts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add ratio for neg sites per transcript? \n",
    "#add greater flank on sides of transcript for neg samples ?\n",
    "def negsOutsidePeaks(genomeFasta,m6A_peaks,m6A_sites,trxts,motif) :\n",
    "    fasta_sequences = SeqIO.parse(open(genomeFasta),'fasta')\n",
    "    seqs={}\n",
    "    for seq in fasta_sequences:\n",
    "        if seq.name[:3]=='chr':\n",
    "            seqs[seq.name]=seq \n",
    "    m6A_peaks = bt.BedTool.from_dataframe(m6A_peaks)\n",
    "    m6A_sites = bt.BedTool.from_dataframe(m6A_sites)\n",
    "    trxts = bt.BedTool.from_dataframe(trxts.loc[:,list(range(0,8))+['trx_name']])\n",
    "    #make sure only use transcripts that overlap with an m6aSite\n",
    "    trxts = trxts.intersect(m6A_sites,wa=True,s=True)\n",
    "    overlaps = trxts.intersect(m6A_peaks,wb=True,wa=True,s=True)\n",
    "    overlaps = overlaps.to_dataframe(sep='\\t',comment='#',header=None).drop_duplicates().groupby(8)\n",
    "    chroms, Ns, trx_names, strands= [],[],[],[]\n",
    "    for sg in overlaps:\n",
    "        sg=sg[1]\n",
    "        trxtRng=pd.Series(range(sg.iloc[0,3]-1+2,sg.iloc[0,4]-2))\n",
    "        pkRngs=list(itertools.chain.from_iterable(\n",
    "            [list(range(sg.iloc[r,10]-1,sg.iloc[r,11])) for r in range(len(sg))]))\n",
    "        trxtRng=list(trxtRng[~trxtRng.isin(pkRngs)])\n",
    "        negSites=[]\n",
    "        chrom = sg.iloc[0,0]\n",
    "        if sg.iloc[0,6] == '+':\n",
    "            for i in trxtRng:\n",
    "                if seqs[chrom].seq[i] == 'A':\n",
    "                    if seqs[chrom].seq[i+1] == 'C' :\n",
    "                        if seqs[chrom].seq[i-1] in ['A','G'] :\n",
    "                            if motif == 'RRACH':\n",
    "                                if seqs[chrom].seq[i-2] in ['A','G'] :\n",
    "                                    if seqs[chrom].seq[i+2] in ['A','C','T'] :\n",
    "                                        negSites.append(i)\n",
    "                            elif motif == 'RAC' :\n",
    "                                negSites.append(i)\n",
    "        else:\n",
    "            for i in trxtRng:\n",
    "                if seqs[chrom].seq[i] == 'T':\n",
    "                    if seqs[chrom].seq[i-1] == 'G' :\n",
    "                        if seqs[chrom].seq[i+1] in ['T','C'] :\n",
    "                            if motif == 'RRACH':\n",
    "                                if seqs[chrom].seq[i+2] in ['T','C'] :\n",
    "                                    if seqs[chrom].seq[i-2] in ['T','G','A'] :\n",
    "                                        negSites.append(i)\n",
    "                            elif motif == 'RAC' :\n",
    "                                negSites.append(i)\n",
    "        lns = len(negSites)\n",
    "        chroms = chroms + [chrom]*lns\n",
    "        Ns = Ns + negSites\n",
    "        trx_names = trx_names + [sg.iloc[0,8]]*lns\n",
    "        strands = strands + [sg.iloc[0,6]]*lns\n",
    "    allNegs = pd.DataFrame([chroms, Ns, pd.Series(Ns)+1, trx_names, [1]*len(Ns), strands]).T\n",
    "    return allNegs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mtxs_toH5Blocks(InpMtxs,TgtMtxs,CHUNK_SIZE,train_or_test):\n",
    "    num_CHUNKS=ceil(len(InpMtxs)/CHUNK_SIZE)\n",
    "    h5f_Inp=h5py.File('Inp_'+train_or_test+'.h5', 'w')\n",
    "    h5f_Tgt=h5py.File('Tgt_'+train_or_test+'.h5', 'w')\n",
    "    for i in range(num_CHUNKS):\n",
    "        Ib=InpMtxs[CHUNK_SIZE*i:CHUNK_SIZE*(i+1)]\n",
    "        Tb=TgtMtxs[CHUNK_SIZE*i:CHUNK_SIZE*(i+1)]\n",
    "        h5f_Inp[str(i)]=np.swapaxes(np.stack(np.array(Ib),axis=1).T,0,1).astype('int8')\n",
    "        h5f_Tgt[str(i)]=np.expand_dims(np.expand_dims(np.array(Tb),1),2).astype('int8')\n",
    "    print ('Inp_'+train_or_test+'.h5'+'and'+'Tgt_'+train_or_test+'.h5'+'created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randSubBlocks(InpBlocks,TgtBlocks):\n",
    "    InpBlocks, TgtBlocks = pd.Series(InpBlocks), pd.Series(TgtBlocks)\n",
    "    rand = np.random.permutation(len(InpBlocks))\n",
    "    InpBlocks = list(InpBlocks.iloc[rand])\n",
    "    TgtBlocks = list(TgtBlocks.iloc[rand])\n",
    "    return InpBlocks,TgtBlocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create multiple resnet layers in model \n",
    "def resnet(x,fltrNumb,fltrW,dilr,numbRns):\n",
    "    def oneLoop(x):\n",
    "        z = BatchNormalization()(x)\n",
    "        z = Activation('relu')(z)\n",
    "        z = Conv1D(fltrNumb,kernel_size=(fltrW,),dilation_rate=dilr,padding='same')(z)\n",
    "        return z\n",
    "    full=x\n",
    "    for n in range(numbRns):\n",
    "        z=oneLoop(full)\n",
    "        z=oneLoop(z)\n",
    "        full = add([full,z]) \n",
    "    return full   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parallel(model, gpu_count):\n",
    "\n",
    "    def get_slice(data, idx, parts):\n",
    "\n",
    "        shape = tf.shape(data)\n",
    "        stride = tf.concat([shape[:1]//parts, shape[1:]*0], 0)\n",
    "        start = stride * idx\n",
    "\n",
    "        size = tf.concat([shape[:1]//parts, shape[1:]], 0) \n",
    "        # Split the batch into equal parts \n",
    "\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    outputs_all = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        outputs_all.append([])\n",
    "\n",
    "    # Place a copy of the model on each GPU, each getting a slice of the batch\n",
    "    for i in range(gpu_count):\n",
    "        with tf.device('/gpu:%d' % i):\n",
    "            with tf.name_scope('tower_%d' % i) as scope:\n",
    "\n",
    "                inputs = []\n",
    "                # Slice each input into a piece for processing on this GPU\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_n = Lambda(get_slice, output_shape=input_shape,\n",
    "                                  arguments={'idx': i, 'parts': gpu_count})(x)\n",
    "                    inputs.append(slice_n)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "                \n",
    "                # Save all the outputs for merging back together later\n",
    "                for l in range(len(outputs)):\n",
    "                    outputs_all[l].append(outputs[l])\n",
    "\n",
    "    # Merge outputs on CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        \n",
    "        merged = []\n",
    "        for outputs in outputs_all:\n",
    "            merged.append(concatenate(outputs, axis=0))\n",
    "            \n",
    "        return Model(inputs=model.inputs, outputs=merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topK_auPRC(model,Inp,Tgt,numOut_Classes):\n",
    "    p=model.predict(Inp)\n",
    "    Tks=[]\n",
    "    PRs=[]\n",
    "    if numOut_Classes > 1:\n",
    "        cRange=range(1,numOut_Classes)\n",
    "    else:\n",
    "        cRange=[0]\n",
    "    for c in cRange:\n",
    "        Fullc=Tgt[:,:,c].flatten()\n",
    "        S=[i for i in range(len(Fullc)) if Fullc[i] == 1]\n",
    "        pS=np.argsort(p[:,:,c].flatten())[-len(S):]\n",
    "        pS=pd.Series(pS)\n",
    "        nCrct=pS[pS.isin(S)].size\n",
    "        Tk=nCrct/float(len(S))\n",
    "        Tks.append(Tk)\n",
    "        PR=average_precision_score(Fullc,p[:,:,c].flatten())\n",
    "        PRs.append(PR)\n",
    "    if numOut_Classes < 3:\n",
    "        Tks.append(0.0)\n",
    "        PRs.append(0.0)\n",
    "    return(Tks[0],Tks[1],PRs[0],PRs[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
