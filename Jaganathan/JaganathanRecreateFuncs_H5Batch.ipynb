{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from keras.models import Sequential, load_model, Model \n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Input, BatchNormalization, Activation, add, MaxPooling1D, Cropping1D \n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "#from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "#import hyperopt.fmin as hypfmin\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from math import ceil\n",
    "from Bio import SeqIO\n",
    "import warnings \n",
    "import datetime\n",
    "import keras.backend as kb\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "import h5py\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.core import Lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select transcripts that are protein coding, principal isoforms, with splice sites \n",
    "def processGff(fn):\n",
    "    GCDE = pd.read_csv(fn,sep='\\t',comment='#',header=None)\n",
    "    pcgBool=[bool(re.search(\"gene_type=protein_coding\",s)) for s in GCDE[8]]\n",
    "    GCDE=GCDE[pcgBool]\n",
    "    GCDE['trx_name']=GCDE.iloc[:,8].str.extract(\"transcript_name=(.+?);\")\n",
    "    GCDE['trx_id']=GCDE.iloc[:,8].str.extract(\"transcript_id=(.+?);\")\n",
    "    GCDE['gene']=GCDE.iloc[:,8].str.extract('gene_name=(.+?);')\n",
    "    trxts, exons = GCDE[GCDE[2]==\"transcript\"], GCDE[GCDE[2]==\"exon\"]\n",
    "    princIso=[bool(re.search(\"appris_principal\",s)) for s in trxts[8]]\n",
    "    trxts=trxts[princIso]\n",
    "    trxts['tsl']=trxts.iloc[:,8].str.extract('transcript_support_level=(.+?);')\n",
    "    trxts['level']=trxts.iloc[:,8].str.extract(';level=(.+?);')\n",
    "    trxts['alt']=[bool(re.search(';tag=.*?(alternative).*?;',s)) for s in trxts[8]]\n",
    "    trxts['trxSz']=trxts[4]-trxts[3]\n",
    "    trxts['tsl']=pd.to_numeric(trxts['tsl'], errors='coerce')\n",
    "    trxts['level']=pd.to_numeric(trxts['level'], errors='coerce')\n",
    "    splitGps = trxts.groupby('gene')\n",
    "    dups=pd.DataFrame()\n",
    "    trxts=pd.DataFrame()\n",
    "    for sg in splitGps:\n",
    "        sg=sg[1]\n",
    "        if len(sg) > 1:\n",
    "            sg=sg[sg['tsl'] == sg['tsl'].min()]\n",
    "            if len(sg) > 1:\n",
    "                sg=sg[sg['level'] == sg['level'].min()]\n",
    "                if len(sg) > 1:\n",
    "                    sg=sg[~sg['alt'].values]\n",
    "                    if len(sg) > 1:\n",
    "                        sg=sg[sg['trxSz'] == sg['trxSz'].max()]\n",
    "                        if len(sg) > 1:\n",
    "                            dups=dups.append(sg)\n",
    "                            continue \n",
    "        trxts=trxts.append(sg)\n",
    "    if len(dups) > 0:\n",
    "        print(\"{} Removed (Still has duplicates!!!)\".format(dups['trx_name']))\n",
    "        print(\"{} gene(s) with duplicates removed!!!\".format(len(set(dups['gene']))))\n",
    "    exons=exons[exons[\"trx_id\"].isin(trxts['trx_id'])]\n",
    "    loneExns=exons.drop_duplicates(subset='trx_id',keep=False).loc[:,\"trx_id\"].values\n",
    "    trxts=trxts[~trxts[\"trx_id\"].isin(loneExns)]\n",
    "    exons=exons[~exons[\"trx_id\"].isin(loneExns)]\n",
    "    cols=list(range(0,9))+['trx_name','gene']\n",
    "    trxts=trxts.loc[:,cols]\n",
    "    exons=exons.loc[:,cols]\n",
    "    return trxts,exons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any genes with pseudogenes\n",
    "def rmPseudoGns(tstTrxts,tstExons,pgsFile):\n",
    "    pgs=pd.read_csv(pgsFile,comment='#')\n",
    "    pgs.dropna(inplace=True)\n",
    "    pgsNms=set(pgs['Human paralogue associated gene name'])\n",
    "    tstTrxts=tstTrxts[~tstTrxts.loc[:,'gene'].isin(pgsNms)]\n",
    "    tstExons=tstExons[~tstExons.loc[:,'gene'].isin(pgsNms)]\n",
    "    return tstTrxts,tstExons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which chromosomes to subset from transcripts and corresponding features \n",
    "def subChroms(trxts,exons,chrs):\n",
    "    chrs=['chr'+str(c) for c in chrs]\n",
    "    trxts=trxts[trxts[0].isin(chrs)].sort_values(0)\n",
    "    exons=exons[exons[0].isin(chrs)]\n",
    "    return trxts,exons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target matrix one-hot encoded for 3 classes: SA,SD,and neither \n",
    "def getTargetMtxs(trxts,InpMtxs,exons):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    tgtMtxs=[]\n",
    "    for r in range(len(trxts)):\n",
    "        row=trxts.iloc[r,:]\n",
    "        trxtExs=exons[exons['trx_name']==row['trx_name']]\n",
    "        trxtExs.sort_values(3,inplace=True)\n",
    "        trxtW=row[4]-row[3]+1\n",
    "        ons=np.matrix(np.ones(trxtW))\n",
    "        zs=np.matrix(np.zeros((2,trxtW)))\n",
    "        tgtMtx=np.concatenate((ons,zs))\n",
    "        if row[6] == '+':\n",
    "            exStarts, exEnds = list(trxtExs[3])[1:], list(trxtExs[4])[:-1]\n",
    "            exStarts=[eS - row[3] for eS in exStarts]\n",
    "            exEnds=[eE - row[3] for eE in exEnds]\n",
    "        else:\n",
    "            exStarts, exEnds = list(trxtExs[4])[:-1], list(trxtExs[3])[1:]\n",
    "            exStarts=[eS - row[3] for eS in exStarts]\n",
    "            exEnds=[eE - row[3] for eE in exEnds]\n",
    "        for eS in exStarts:\n",
    "            try:\n",
    "                tgtMtx[1,eS] = 1\n",
    "                tgtMtx[0,eS] = 0\n",
    "            except:\n",
    "                print(row)\n",
    "        for eE in exEnds:\n",
    "            try:\n",
    "                tgtMtx[2,eE] = 1\n",
    "                tgtMtx[0,eE] = 0\n",
    "            except:\n",
    "                return row\n",
    "        if row[6] == '-' :\n",
    "            tgtMtx = np.fliplr(tgtMtx)\n",
    "        InMx = np.array(InpMtxs[r])\n",
    "        Nis = [Ni for Ni in range(InMx.shape[1]) if (InMx[:,Ni] == [[0],[0],[0],[0]]).all()]\n",
    "        if len(Nis)>0:\n",
    "            for Ni in Nis:\n",
    "                tgtMtx[:,Ni]=[[0],[0],[0]]   \n",
    "        tgtMtxs.append(tgtMtx)\n",
    "    return tgtMtxs\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get transcript sequences into one-hot encoded form \n",
    "def getInputMtxs(genomeFasta,trxts):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    fasta_sequences = SeqIO.parse(open(genomeFasta),'fasta')\n",
    "    seqs={}\n",
    "    for seq in fasta_sequences:\n",
    "        if seq.name[:3]=='chr':\n",
    "            seqs[seq.name]=seq  \n",
    "    trnMtxs=[]\n",
    "    posBases=dict(zip(['A','C','G','T'],range(4)))\n",
    "    negBases=dict(zip(['T','G','C','A'],range(4)))\n",
    "    for r in range(len(trxts)):\n",
    "        row=trxts.iloc[r,:]\n",
    "        trxtW=row[4]-row[3]+1\n",
    "        trxtSeq=seqs[row[0]].seq[row[3]-1:row[4]]\n",
    "        trnMtx = np.matrix(np.zeros((4,trxtW)))\n",
    "        if row[6] == '+':\n",
    "            for bi in range(trxtW):\n",
    "                try:\n",
    "                    b = trxtSeq[bi]\n",
    "                    trnMtx[posBases[b],bi] = 1\n",
    "                except:\n",
    "                    continue \n",
    "        else:\n",
    "            for bi in range(trxtW):        \n",
    "                try:\n",
    "                    b = trxtSeq[bi]            \n",
    "                    trnMtx[negBases[b],bi] = 1\n",
    "                except:\n",
    "                    continue \n",
    "            trnMtx = np.fliplr(trnMtx)\n",
    "        trnMtxs.append(trnMtx)\n",
    "    return trnMtxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S/2 + I + S/2 sized blocks to feed into model where S is overall size of flank and I is sequences to predict status \n",
    "def getInputBlocks(trnMtxs,flankL):\n",
    "    trnBlocks=[]\n",
    "    halfFlankL=int(flankL/2)\n",
    "    for trnMtx in trnMtxs:\n",
    "        numBlocks = ceil(len(trnMtx.T)/5000)\n",
    "        blW = numBlocks * 5000\n",
    "        bAdd = blW - len(trnMtx.T) \n",
    "        rightPad=int(halfFlankL + bAdd)\n",
    "        trnMtxZpd = np.pad(trnMtx,((0,0),(halfFlankL,rightPad)),'constant')\n",
    "        for bln in range(numBlocks):\n",
    "            blLow = 5000*(bln) \n",
    "            blHigh = 5000*(bln+1) + flankL # - 1 in blHigh is omitted  \n",
    "            block = trnMtxZpd[:,blLow:blHigh]\n",
    "            trnBlocks.append(block)\n",
    "    return trnBlocks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target blocks in size I blocks \n",
    "def getTargetBlocks(tgtMtxs):\n",
    "    tgtBlocks=[]\n",
    "    for tgtMtx in tgtMtxs:\n",
    "        numBlocks = ceil(len(tgtMtx.T)/5000)\n",
    "        blW = numBlocks * 5000\n",
    "        bAdd = blW - len(tgtMtx.T) \n",
    "        tgtMtxZpd = np.pad(tgtMtx,((0,0),(0,bAdd)),'constant')\n",
    "        for bln in range(numBlocks):\n",
    "            blLow = 5000*(bln) \n",
    "            blHigh = 5000*(bln+1) # - 1 in blHigh is omitted  \n",
    "            block = tgtMtxZpd[:,blLow:blHigh]\n",
    "            tgtBlocks.append(block)\n",
    "    return tgtBlocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mtxs_toH5Blocks(InpMtxs,TgtMtxs,flankL,CHUNK_SIZE,train_or_test):\n",
    "    #assert len(InpMtxs)==len(TgtMtxs)\n",
    "    num_CHUNKS=ceil(len(InpMtxs)/CHUNK_SIZE)\n",
    "    h5f_Inp=h5py.File('Inp_'+train_or_test+'.h5', 'w')\n",
    "    h5f_Tgt=h5py.File('Tgt_'+train_or_test+'.h5', 'w')\n",
    "    for i in range(num_CHUNKS):\n",
    "        Ib=getInputBlocks(InpMtxs[CHUNK_SIZE*i:CHUNK_SIZE*(i+1)],flankL)\n",
    "        Tb=getTargetBlocks(TgtMtxs[CHUNK_SIZE*i:CHUNK_SIZE*(i+1)])\n",
    "        h5f_Inp[str(i)]=np.swapaxes(np.stack(Ib,axis=1).T,0,1).astype('int8')\n",
    "        h5f_Tgt[str(i)]=np.swapaxes(np.stack(Tb,axis=1).T,0,1).astype('int8')\n",
    "    print ('Inp_'+train_or_test+'.h5'+'and'+'Tgt_'+train_or_test+'.h5'+'created')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create multiple resnet layers in model \n",
    "def resnet(x,fltrNumb,fltrW,dilr,numbRns):\n",
    "    def oneLoop(x):\n",
    "        z = BatchNormalization()(x)\n",
    "        z = Activation('relu')(z)\n",
    "        z = Conv1D(fltrNumb,kernel_size=(fltrW,),dilation_rate=dilr,padding='same')(z)\n",
    "        return z\n",
    "    full=x\n",
    "    for n in range(numbRns):\n",
    "        z=oneLoop(full)\n",
    "        z=oneLoop(z)\n",
    "        full = add([full,z]) \n",
    "    return full   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard categorical cross entropy for sequence outputs\n",
    "def categorical_crossentropy_2d(y_true, y_pred):\n",
    "    return - kb.mean(y_true[:, :, 0]*kb.log(y_pred[:, :, 0]+1e-10)\n",
    "                   + y_true[:, :, 1]*kb.log(y_pred[:, :, 1]+1e-10)\n",
    "                   + y_true[:, :, 2]*kb.log(y_pred[:, :, 2]+1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parallel(model, gpu_count):\n",
    "\n",
    "    def get_slice(data, idx, parts):\n",
    "\n",
    "        shape = tf.shape(data)\n",
    "        stride = tf.concat([shape[:1]//parts, shape[1:]*0], 0)\n",
    "        start = stride * idx\n",
    "\n",
    "        size = tf.concat([shape[:1]//parts, shape[1:]], 0) \n",
    "        # Split the batch into equal parts \n",
    "\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    outputs_all = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        outputs_all.append([])\n",
    "\n",
    "    # Place a copy of the model on each GPU, each getting a slice of the batch\n",
    "    for i in range(gpu_count):\n",
    "        with tf.device('/gpu:%d' % i):\n",
    "            with tf.name_scope('tower_%d' % i) as scope:\n",
    "\n",
    "                inputs = []\n",
    "                # Slice each input into a piece for processing on this GPU\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_n = Lambda(get_slice, output_shape=input_shape,\n",
    "                                  arguments={'idx': i, 'parts': gpu_count})(x)\n",
    "                    inputs.append(slice_n)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "                \n",
    "                # Save all the outputs for merging back together later\n",
    "                for l in range(len(outputs)):\n",
    "                    outputs_all[l].append(outputs[l])\n",
    "\n",
    "    # Merge outputs on CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        \n",
    "        merged = []\n",
    "        for outputs in outputs_all:\n",
    "            merged.append(concatenate(outputs, axis=0))\n",
    "            \n",
    "        return Model(inputs=model.inputs, outputs=merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topK_auPRC(model,Inp,Tgt):\n",
    "    p=model.predict(Inp)\n",
    "    Tks=[]\n",
    "    PRs=[]\n",
    "    for c in [1,2]:\n",
    "        Fullc=Tgt[:,:,c].flatten()\n",
    "        S=[i for i in range(len(Fullc)) if Fullc[i] == 1]\n",
    "        pS=np.argsort(p[:,:,c].flatten())[-len(S):]\n",
    "        pS=pd.Series(pS)\n",
    "        nCrct=pS[pS.isin(S)].size\n",
    "        Tk=nCrct/len(S)\n",
    "        Tks.append(Tk)\n",
    "        PR=average_precision_score(Fullc,p[:,:,c].flatten())\n",
    "        PRs.append(PR)\n",
    "    return(Tks[0],Tks[1],PRs[0],PRs[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
